{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network with the beautiful MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game plan:\n",
    "\n",
    "\t\t○ Import libraries, acquire data, and pre-process it\n",
    "        ○ Outline the model and choose the activation functions we want to employ\n",
    "\t\t○ Describe placeholders, variables, and related operations\n",
    "\t\t○ Choose the appropriate advanced optimisers\n",
    "\t\t○ Split data set into batches for faster learning\n",
    "\t\t○ Initialise the variables\n",
    "\t\t○ Make the model learn\n",
    "        ○ Test accuracy of the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70,000 images of handwritten digits. The goal is to build an algorithm that classifies them correctly (0-9). Each photo is 28x28 pixels, so can think about the problem as a 28x28 matrix with values from 0 to 1. The approach for deep neural networks is to \"flatten\" each 28x28 image into a vector of 784x1. Each image would have 784 inputs, each pixel is an input for our neural network. Each pixel corresponds to the intensity of colour (0:completely white to 1:completely black). For this example, there will be 784 inputs, 2 hidden layers (enough for good accuracy), and 10 outputs (numbers 0-9). One-hot encoding for the outputs and the targets. Since we want to see probabilities of digits being accurately labelled, we will use softmax activation function for the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#import the datasets where MNIST will be imported from\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads the MNIST data set from the tensorflow datasets\n",
    "# as_supervised = True: loads the dataset in a 2-tuple structure [input,target]\n",
    "# mnist_info and with_info = True: provides a tuple containing info about version, features, number of samples in dataset\n",
    "mnist_dataset, mnist_info = tfds.load(\"mnist\", with_info = True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the train and test datasets from MNIST\n",
    "# note that MNIST does not provide a validation dataset, need to split the train dataset further\n",
    "mnist_train, mnist_test = mnist_dataset[\"train\"], mnist_dataset [\"test\"]\n",
    "\n",
    "# let's take 10% of the train dataset to serve as validation, get the number of training samples /10\n",
    "num_validation_samples = 0.1 * mnist_info.splits[\"train\"].num_examples\n",
    "\n",
    "# however, this might not be an integer\n",
    "# can use tf.cast to convert the validation variable into an integer, preventing any issues\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# also need an easy acces variable for the number of test samples, using splits as above and cast it to an integer\n",
    "num_test_samples = mnist_info.splits[\"test\"].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# also need to scale the inputs to make the model more numerically stable, taking the MNIST image and label\n",
    "# currently the values are represented from 0 to 255 (the scale of grey), aim is to make it fro 0 to 1\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32) #covert to float just in case\n",
    "    image /= 255. #divide all numbers by 255 to get the scale from 0 to 1 as a float\n",
    "    return image, label\n",
    "\n",
    "# create new scaled variable with the function applied\n",
    "# .map(function) applies a custom transformation to the dataset\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# similarly, need to scale the test data set as well\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "# next step is to shuffle the data so that it is impossible for it to be ordered which will confuse the model\n",
    "# set the buffer size since dealing with the whole dataset will be too much, I'll set the batch size to 10,000 at a time\n",
    "\n",
    "buffer_size = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "# the need to extract the train and validation data into their own variables with .take and .skip\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples) #takes num validation samples\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples) #takes everything BUT num validation samples\n",
    "\n",
    "# since I will be using mini-batch gradient descent to train the model so need to specify batch size and prepare data\n",
    "batch_size = 150\n",
    "# use the .batch method to combine the elements of a dataset into batches \n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# as for the validation data, don't need to separate into batches because there will be no backpropogation\n",
    "# but the model will expect the validation in batch form too, so will overwrite the validation data with num of samples\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# same with the test data, don't need to batch it but the model will expect a batch form\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# validation data must have the same properties as the train and test data\n",
    "# iter is to have an object that can be iterated one element at a time\n",
    "# next will load the next element of the iter object\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declate the input, output and for the neural network model\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 1000 #arbitrary, need to do testing to find best width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model, flattening the tensors into vectors, softmax will produce probability outputs\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape = (28,28,1)), #flatten to vector, input layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"), #first hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"), #second hidden layer\n",
    "                            tf.keras.layers.Dense(output_size, activation=\"softmax\") # output layer    \n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must specify the optimiser and loss using the .compile method. Adam is the best optimizer at the moment\n",
    "# for loss, need to use crossentropy, and will use sparse categorical crossentropy to apply one-hot encoding\n",
    "# add metrics to help configure the model for training, which is accuracy\n",
    "model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "360/360 - 36s - loss: 0.2045 - accuracy: 0.9377 - val_loss: 0.0834 - val_accuracy: 0.9748\n",
      "Epoch 2/5\n",
      "360/360 - 37s - loss: 0.0774 - accuracy: 0.9760 - val_loss: 0.0689 - val_accuracy: 0.9810\n",
      "Epoch 3/5\n",
      "360/360 - 36s - loss: 0.0490 - accuracy: 0.9844 - val_loss: 0.0507 - val_accuracy: 0.9835\n",
      "Epoch 4/5\n",
      "360/360 - 37s - loss: 0.0378 - accuracy: 0.9874 - val_loss: 0.0350 - val_accuracy: 0.9893\n",
      "Epoch 5/5\n",
      "360/360 - 36s - loss: 0.0290 - accuracy: 0.9903 - val_loss: 0.0347 - val_accuracy: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1728ffb7cc8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the number of epochs (iterations), chosen arbitrarily for now\n",
    "num_epochs = 5\n",
    "\n",
    "# fit the model and add validation data, verbose = 2 to receive only the most important info for each epoch\n",
    "model.fit(train_data, epochs = num_epochs, validation_data = (validation_inputs, validation_targets), validation_steps=10, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 7s 7s/step - loss: 0.0759 - accuracy: 0.9803"
     ]
    }
   ],
   "source": [
    "# can use .evaluate to return the loss value and metrics values for the model in testing mode\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 98.03%\n"
     ]
    }
   ],
   "source": [
    "# apply formatting\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3.7_TF2.0]",
   "language": "python",
   "name": "conda-env-python3.7_TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
